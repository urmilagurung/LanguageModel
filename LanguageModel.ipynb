{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKfbUzSp5dlR",
    "outputId": "72e50064-3e25-45a0-db55-7533f702af06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Uploading the dataset\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVfMLhRD5fH_",
    "outputId": "f1810660-b064-402f-fbd2-e91c38551f9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/DL_Coursework/Task3\n"
     ]
    }
   ],
   "source": [
    "%cd \"gdrive/My Drive/DL_Coursework/Task3/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TjHTV10l5p9k"
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import os\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rbRI_Mtx5xCG",
    "outputId": "6c9c3720-a392-411e-a6ad-edefdca87104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The archive has 149326361 characters\n"
     ]
    }
   ],
   "source": [
    "# Load the data and create a dataset\n",
    "with open('internet_archive_scifi_v3.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Print the length of text\n",
    "print('The archive has {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k9-Tf3As5rb0",
    "outputId": "5ac8117d-4dfe-4101-a4bf-4c7d73b1d166"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In two minutes, he slumped forward on the desk. His death precipitated the great stock market crash \n"
     ]
    }
   ],
   "source": [
    "# Select text between indices\n",
    "text_content = text[580:149322961]\n",
    "print(text_content[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XKxCTe5Jy_sL",
    "outputId": "ec9e3671-53f8-4f8e-b4a8-fd70b275be49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The archive has 149294026 characters\n"
     ]
    }
   ],
   "source": [
    "sub_text = text_content.replace(\"  \", \" \")\n",
    "\n",
    "# Print the length of text\n",
    "print('The archive has {} characters'.format(len(sub_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EjYwCwG73rwr",
    "outputId": "6e19ea00-f28e-449a-b20e-26e144d4af2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The archive are 75 unique characters\n"
     ]
    }
   ],
   "source": [
    "# Print the unique characters\n",
    "vocab = sorted(set(sub_text))\n",
    "print('The archive are {} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMEyAgFFzZuO",
    "outputId": "8f3cfbf9-aab0-4048-eaf4-47935ca9e6d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In two minutes, he slumped forward on the desk. His death precipitated the great stock market crash \n"
     ]
    }
   ],
   "source": [
    "print(sub_text[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-r77sVTsFc9m"
   },
   "outputs": [],
   "source": [
    "# Character to index\n",
    "chartoindex = {v:i for i,v in enumerate(vocab)}\n",
    "int_text = np.array([chartoindex[i] for i in sub_text])\n",
    "\n",
    "# Index to character \n",
    "indextochar = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zkm3OmBBFt_m",
    "outputId": "0e1d3d93-de00-477a-a332-cf93cca1f028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character to index: \n",
      "\n",
      "  ' ' :   0\n",
      "  '!' :   1\n",
      "  '\"' :   2\n",
      "  '#' :   3\n",
      "  \"'\" :   4\n",
      "  '(' :   5\n",
      "  ')' :   6\n",
      "  ',' :   7\n",
      "  '-' :   8\n",
      "  '.' :   9\n",
      "  '0' :  10\n",
      "  '1' :  11\n",
      "  '2' :  12\n",
      "  '3' :  13\n",
      "  '4' :  14\n",
      "  '5' :  15\n",
      "  '6' :  16\n",
      "  '7' :  17\n",
      "  '8' :  18\n",
      "  '9' :  19\n",
      "  ':' :  20\n",
      "  ';' :  21\n",
      "  '?' :  22\n",
      "  'A' :  23\n",
      "  'B' :  24\n",
      "  'C' :  25\n",
      "  'D' :  26\n",
      "  'E' :  27\n",
      "  'F' :  28\n",
      "  'G' :  29\n",
      "  'H' :  30\n",
      "  'I' :  31\n",
      "  'J' :  32\n",
      "  'K' :  33\n",
      "  'L' :  34\n",
      "  'M' :  35\n",
      "  'N' :  36\n",
      "  'O' :  37\n",
      "  'P' :  38\n",
      "  'Q' :  39\n",
      "  'R' :  40\n",
      "  'S' :  41\n",
      "  'T' :  42\n",
      "  'U' :  43\n",
      "  'V' :  44\n",
      "  'W' :  45\n",
      "  'X' :  46\n",
      "  'Y' :  47\n",
      "  'Z' :  48\n",
      "  'a' :  49\n",
      "  'b' :  50\n",
      "  'c' :  51\n",
      "  'd' :  52\n",
      "  'e' :  53\n",
      "  'f' :  54\n",
      "  'g' :  55\n",
      "  'h' :  56\n",
      "  'i' :  57\n",
      "  'j' :  58\n",
      "  'k' :  59\n",
      "  'l' :  60\n",
      "  'm' :  61\n",
      "  'n' :  62\n",
      "  'o' :  63\n",
      "  'p' :  64\n",
      "  'q' :  65\n",
      "  'r' :  66\n",
      "  's' :  67\n",
      "  't' :  68\n",
      "  'u' :  69\n",
      "  'v' :  70\n",
      "  'w' :  71\n",
      "  'x' :  72\n",
      "  'y' :  73\n",
      "  'z' :  74\n",
      "\n",
      " Text to integer: \n",
      "\n",
      "'science fiction maga' to [67 51 57 53 62 51 53  0 54 57 51 68 57 63 62  0 61 49 55 49]\n"
     ]
    }
   ],
   "source": [
    "print(\"Character to index: \\n\")\n",
    "for char,_ in zip(chartoindex, range(105)):\n",
    "    print('  {:4s}: {:3d}'.format(repr(char), chartoindex[char]))\n",
    "\n",
    "print(\"\\n Text to integer: \\n\")\n",
    "print('{} to {}'.format(repr(sub_text[:20]),int_text[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "l11cnVOsFxZd"
   },
   "outputs": [],
   "source": [
    "# Maximum characters as an input\n",
    "length= 80   # sequence length\n",
    "examples_per_epoch = len(sub_text)\n",
    "\n",
    "# Text to character index stream\n",
    "char_dt = tf.data.Dataset.from_tensor_slices(int_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Tbbm4sDjF51q"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Sequences from individual characters\n",
    "sequences = char_dt.batch(length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FbBgp6HqF8Yb"
   },
   "outputs": [],
   "source": [
    "# Create input-target pairs\n",
    "def input_target_pairs(k):\n",
    "    input_text = k[:-1]\n",
    "    target_text = k[1:]          # next step\n",
    "    return input_text, target_text\n",
    "\n",
    "data_text = sequences.map(input_target_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "r5lk7UlneHEJ"
   },
   "outputs": [],
   "source": [
    "# Batch size, buffer size for shuffling\n",
    "batch_size = 128\n",
    "buffer_size = 10000\n",
    "\n",
    "dataset = data_text.shuffle(buffer_size).batch(batch_size , drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Psba6eSAGA2k",
    "outputId": "d1824395-2c5b-4ad5-9a7b-5ede61d8c0ac",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character_Stream: \n",
      "\n",
      "s\n",
      "c\n",
      "i\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      " \n",
      "f\n",
      "i\n",
      "c\n",
      "t\n",
      "i\n",
      "\n",
      "Sequence: \n",
      "\n",
      "'science fiction magazine called IF. The title was selected after much thought bec'\n",
      "'ause of its brevity and on the theory it is indicative of the field and will be e'\n",
      "\"asy to remember. The tentative title that just morning and couldn't remember it u\"\n",
      "\"ntil we'd had a cup of coffee, it was summarily discarded. A great deal of though\"\n",
      "'t and effort lias gone into the formation of this magazine. We have had the aid o'\n",
      "'f several very talented and generous people, for which we are most grateful. Much'\n",
      "' is due them for their warmhearted assistance. And now that the bulk of the forma'\n",
      "'tive work is done, we will try to maintain IF as one of the finest books on the m'\n",
      "'arket. t a great public demand for our magazine. In short, why will you buy IF? W'\n",
      "'e cannot, in honesty, say we will publish at all times the best science fiction i'\n",
      "'n the field. That would not be true. But we will have access to the best stories,'\n",
      "' and we will get our fair share of works from the best writers. We definitely wil'\n",
      "'l not talk \"adult\" or \"juvenile\" relative to our content as we feel such terms ar'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Character_Stream: \\n\")\n",
    "for i in char_dt.take(13):\n",
    "  print(indextochar[i.numpy()])  \n",
    "\n",
    "print(\"\\nSequence: \\n\")\n",
    "for i in sequences.take(13):\n",
    "  print(repr(''.join(indextochar[i.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "sd77cO0hGBIU"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set the seed\n",
    "random.seed(100)\n",
    "# Create the lstm model\n",
    "def model_lstm(vocabul_size, embedding_dim, rnn_units, batch_size):\n",
    "    txt_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocabul_size, embedding_dim,           # embedding\n",
    "                              batch_input_shape=[batch_size, None]), \n",
    "    tf.keras.layers.LSTM(rnn_units,                                  # LSTM\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,),\n",
    "    tf.keras.layers.Dense(vocabul_size)                              # Dense\n",
    "  ])\n",
    "    return txt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DHUDdKYTGHYW"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 256\n",
    "rnn_units= 1024\n",
    "\n",
    "lstm_txt_model = model_lstm(vocabul_size = vocab_size,embedding_dim=embedding_dim, rnn_units=rnn_units,batch_size=batch_size)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJu7-hgsGJD_",
    "outputId": "4ac941f3-aaaf-4ce7-a2d6-d016f9f0c314"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 80, 75)\n"
     ]
    }
   ],
   "source": [
    "# Test the shape\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    prediction = lstm_txt_model(input_example_batch)\n",
    "    assert (prediction.shape == (batch_size, length, vocab_size)) \n",
    "    print(prediction.shape)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IvtHAXP--B9",
    "outputId": "428f703c-751b-4949-a60a-dd229b59ffac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (128, 80, 75)\n",
      "Loss:       4.3179383\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# Test the loss\n",
    "example_loss  = loss(target_example_batch, prediction)\n",
    "print(\"Prediction shape: \", prediction.shape)\n",
    "print(\"Loss:      \", example_loss.numpy().mean())\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "wSyyEzJAGKv7"
   },
   "outputs": [],
   "source": [
    "adam_Optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001) # Initializing Learning Rate\n",
    "\n",
    "# Compile the model\n",
    "lstm_txt_model.compile(optimizer=adam_Optimizer, loss=loss, metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ISZ9NEEwGUku"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Model checkpoints\n",
    "lstm_dir_checkpoints = 'lstm_checkpoints'\n",
    "checkpoint_prefix = os.path.join(lstm_dir_checkpoints, \"checkpt_{epoch}\") \n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True, monitor= 'loss', save_best_only=True)\n",
    "\n",
    "# create the EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XCwNeIrGOmm",
    "outputId": "125acaa9-ed1f-4c89-a77a-c045fb49534c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = lstm_txt_model.fit(dataset, epochs=15, callbacks=[checkpoint_callback, early_stopping]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Vi7ELnkXGX_i"
   },
   "outputs": [],
   "source": [
    "lstm_mod = model_lstm(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "lstm_mod.load_weights('lstm_checkpoints/checkpt_10').expect_partial()\n",
    "lstm_mod.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "7nEmRZqYGeQO"
   },
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "def generate_text(model, input_string, num=200, temperature=0.5, wrap_width=80):\n",
    "    input_indices = [chartoindex[s] for s in input_string]  # text to indexes\n",
    "    input_indices = tf.expand_dims(input_indices, 0)\n",
    "\n",
    "    # result with predicted characters\n",
    "    text_result = []\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num):\n",
    "        predictions = model(input_indices)\n",
    "        # Remove the dimension from batch\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Categorical distribution for prediction\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Pass the predicted character\n",
    "        input_indices = tf.expand_dims([predicted_id], 0)\n",
    "        char = indextochar[predicted_id]\n",
    "        text_result.append(char)\n",
    "        \n",
    "        # Add line break after each complete word\n",
    "        if char == ' ':\n",
    "            word = ''.join(text_result).rsplit(' ', 1)[1]\n",
    "            if len(word) > 1 and word[-1] not in ['.', '!', '?']:\n",
    "                text_result.append('\\n')\n",
    "\n",
    "    # Join the characters and wrap the text\n",
    "    generated_text = ''.join(text_result)\n",
    "    wrapped_text = textwrap.fill(generated_text, wrap_width)\n",
    "\n",
    "    return input_string + wrapped_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uhhqxEe8OHkn",
    "outputId": "51642044-6dd7-4ed0-f8df-4299e3a03cc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your text: The moon revolves around\n",
      "\n",
      "Prediction:\n",
      "The moon revolves around the world of the Earth which is all the continuing ones of the same as a\n",
      "possibility of the enginee\n"
     ]
    }
   ],
   "source": [
    "# User Input \n",
    "lstm_pred = input(\"Enter your text: \")\n",
    "\n",
    "print('\\nPrediction:')\n",
    "# Prediction\n",
    "print(generate_text(model = lstm_mod, input_string=lstm_pred, num=100, temperature=0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1u-8UfZGg9y",
    "outputId": "bae2ae07-10b8-4da9-9ec3-15b7f2fe753e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your text: I want a trip to space \n",
      "\n",
      "Prediction:\n",
      "I want a trip to space is a considerable man of some other person. The word of the man who had been a\n",
      "mechanical account of the problem of the Earth in the present time of the man\n",
      "who had already been a statement with the real reason of the world of the\n",
      "construction of the pyramid of the Earth. The stories are probably a planet of\n",
      "the position of the orbit of Mars. And then they can be the first of the\n",
      "centuries of the pyramid that has no doubt that it will be a primitive one. The\n",
      "problem was that the interest in the world was surprised to see the medical\n",
      "survival of the problem. It was a different way. The whole thing was a science\n",
      "fiction story on the Skyroval Confederation of the Moon. The first time the body\n",
      "was the only planet of the complete reason. He was a bigger than the other woman\n",
      "who was a shadow of a second on the other side of the compartment. \"It was a\n",
      "long time ago, I guess I can't see the dead songs of the sight of the wind. The\n",
      "words were almost as good as a transport term, and the sound of\n"
     ]
    }
   ],
   "source": [
    "# User Input \n",
    "lstm_pred = input(\"Enter your text: \")\n",
    "\n",
    "print('\\nPrediction:')\n",
    "# Prediction\n",
    "print(generate_text(model = lstm_mod, input_string=lstm_pred, num=1000, temperature=0.4))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
